{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import imagehash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_file_meta_df(df_meta_files):\n",
    "    df_meta_files = df_meta_files[df_meta_files['is_file']==True]\n",
    "    df_meta_files['ext_lower'] = df_meta_files['ext'].str.lower()\n",
    "    # print(df_meta_files.ext_lower.value_counts())\n",
    "    selected_ext = [\n",
    "        \".jpg\", \n",
    "        \".heic\",\n",
    "        \".png\", \n",
    "        \".mov\", \n",
    "        \".avi\", \n",
    "        \".aae\", \n",
    "        \".bmp\", \n",
    "        \".mp4\", \n",
    "        \".mts\", \n",
    "        \".gif\", \n",
    "        \".3gp\", \n",
    "        \".mpg\", \n",
    "        \".mp3\", \n",
    "        \".3gp\", \n",
    "        \".jpeg\",\n",
    "        \".flv\",\n",
    "        \".wmv\",\n",
    "        \".wav\",\n",
    "        \".webp\",\n",
    "        \".m4v\",\n",
    "    ]\n",
    "\n",
    "    df_meta_file_media = df_meta_files[df_meta_files[\"ext_lower\"].isin(set(selected_ext))]\n",
    "    total_media_size = df_meta_file_media['size'].sum()\n",
    "    total_file_size = df_meta_files['size'].sum()\n",
    "    print('total media size:\\t', total_media_size)\n",
    "    print('total file size:\\t', total_file_size)\n",
    "    print('media-size / file-size:\\t', total_media_size/total_file_size)\n",
    "    \n",
    "    return df_meta_file_media.to_dict(\"records\")\n",
    "\n",
    "\n",
    "\n",
    "def get_dict(records):\n",
    "    d = {}\n",
    "    for re in records:\n",
    "        fname = re['fname']\n",
    "        if fname not in d.keys():\n",
    "            d[fname] = []\n",
    "        d[fname].append(re)\n",
    "    \n",
    "    for key in d.keys():\n",
    "        d[key] = sorted(d[key], key=lambda x: x['mtime'])\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def is_same(record1, record2):\n",
    "    # is two file the same?\n",
    "    is_fname_same = record1['fname'] == record2['fname']\n",
    "    # is_size_same = record1['size'] == record2['size']\n",
    "    is_size_same = (abs(record1['size'] - record2['size']) <= 10)\n",
    "    \n",
    "    # is_ctime_yyyy_same = record1['ctime_YYYY'] == record2['ctime_YYYY']\n",
    "    # is_ctime_mm_same = record1['ctime_MM'] == record2['ctime_MM']\n",
    "    # is_ctime_dd_same = record1['ctime_DD'] == record2['ctime_DD']\n",
    "    # is_ctime_same = (is_ctime_yyyy_same and is_ctime_mm_same and is_ctime_dd_same)\n",
    "    \n",
    "    # is_ctime_same = record1['ctime'] == record2['ctime']\n",
    "    # is_mtime_same = record1['mtime'] == record2['mtime']\n",
    "\n",
    "    is_mtime_exact_same = record1['mtime'] == record2['mtime']\n",
    "    \n",
    "    is_mtime_yyyy_same = record1['mtime_YYYY'] == record2['mtime_YYYY']\n",
    "    is_mtime_mm_same = record1['mtime_MM'] == record2['mtime_MM']\n",
    "    is_mtime_dd_same = record1['mtime_DD'] == record2['mtime_DD']\n",
    "    is_mtime_same = (is_mtime_yyyy_same and is_mtime_mm_same and is_mtime_dd_same)\n",
    "    is_img_hash_similar = False\n",
    "    \n",
    "    img_hash_str_1 = record1['img_hash_str']\n",
    "    img_hash_str_2 = record2['img_hash_str']\n",
    "    if img_hash_str_1 == \"\" or img_hash_str_2 == \"\":\n",
    "        is_img_hash_similar =  True\n",
    "    else:\n",
    "        # if info1['Image Height'] == info2['Image Height'] and info1['Image Width'] == info2['Image Width']:\n",
    "        #     is_metadata_same = True\n",
    "        if (imagehash.hex_to_hash(img_hash_str_1)  - imagehash.hex_to_hash(img_hash_str_2) )< 5: # bit hamming distance\n",
    "            is_img_hash_similar = True\n",
    "    \n",
    "    return is_fname_same and (is_mtime_exact_same or is_img_hash_similar or is_size_same) \n",
    "    \n",
    "\n",
    "def resolve_fname_conflict(records):\n",
    "    total_records = len(records)\n",
    "    if total_records == 1:\n",
    "        return records\n",
    "        \n",
    "    status = [-1]*total_records\n",
    "    status[0] = 1 # 1: keep # -1: not determine #0: remove\n",
    "    \n",
    "    \n",
    "    for i in range(total_records):\n",
    "        for j in range(i+1,total_records):\n",
    "            record_i = records[i]\n",
    "            record_j = records[j]\n",
    "            \n",
    "            if status[j]==0:\n",
    "                # j is deleted, nothing to compare, do not make change\n",
    "                continue\n",
    "            \n",
    "            is_same_val = is_same(record_i, record_j)\n",
    "            # print(record_i['size'],record_j['size'],is_same_val)\n",
    "            \n",
    "            if is_same_val:\n",
    "                # delete\n",
    "                status[j] = 0\n",
    "            else:\n",
    "                # keep so far\n",
    "                status[j] = 1\n",
    "    \n",
    "    deduplicate_records = []\n",
    "    \n",
    "    for keep, record in zip(status, records):\n",
    "        if keep == 1:\n",
    "            deduplicate_records.append(record)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return deduplicate_records\n",
    "\n",
    "\n",
    "def merge_dict(dict_old, dict_google):\n",
    "    # merge two into one big dictionary, then resolve fname\n",
    "    len_google_key = len(dict_google.keys())\n",
    "    len_not_same_name = 0\n",
    "    len_same_name = 0\n",
    "    for old_key in tqdm(dict_old.keys()):\n",
    "        if old_key in dict_google.keys():\n",
    "            old_records = dict_old[old_key]\n",
    "            dict_google[old_key]+=old_records\n",
    "            len_same_name+=1\n",
    "        else:\n",
    "            dict_google[old_key]=dict_old[old_key]\n",
    "            len_not_same_name+=1\n",
    "        prev_records = dict_google[old_key]\n",
    "        dict_google[old_key] = resolve_fname_conflict(prev_records)\n",
    "    \n",
    "    len_google_key_new = len(dict_google.keys())\n",
    "    print(f'#google key: old:{len_google_key}, new: {len_google_key_new}, added: {len_google_key_new - len_google_key}')\n",
    "    print('len_not_same_name', len_not_same_name)\n",
    "    print('len_same_name', len_same_name)\n",
    "    return dict_google\n",
    "\n",
    "\n",
    "def merge_files(rcd_old, rcd_google):\n",
    "    # get the dict of list\n",
    "    dict_old = get_dict(rcd_old)\n",
    "    dict_google = get_dict(rcd_google)\n",
    "    print('old #fname', len(dict_old.keys()))\n",
    "    old_records = sum([len(dict_old[key]) for key in dict_old.keys()])\n",
    "    print('old #rcrds', old_records)\n",
    "    \n",
    "    print('google #fname', len(dict_google.keys()))\n",
    "    google_records = sum([len(dict_google[key]) for key in dict_google.keys()])\n",
    "    print('google #rcrds', google_records)\n",
    "    \n",
    "    print('total #rcrd', old_records+google_records)\n",
    "\n",
    "    # self-deduplication\n",
    "    dedup_num_rcds_old = 0 \n",
    "    dedup_num_rcds_google = 0\n",
    "    for key in tqdm(dict_old.keys()):\n",
    "        prev_records = dict_old[key]\n",
    "        prev_num_rcds = len(prev_records)\n",
    "        dict_old[key] = resolve_fname_conflict(prev_records)\n",
    "        curt_num_rcds = len(dict_old[key])\n",
    "        dedup_num_rcds_old += (prev_num_rcds - curt_num_rcds)\n",
    "    for key in tqdm(dict_google.keys()):\n",
    "        prev_records = dict_google[key]\n",
    "        prev_num_rcds = len(prev_records)\n",
    "        dict_google[key] = resolve_fname_conflict(dict_google[key])\n",
    "        curt_num_rcds = len(dict_google[key])\n",
    "        dedup_num_rcds_google += (prev_num_rcds - curt_num_rcds)\n",
    "        \n",
    "    # for the same size.... how to remove them ? \n",
    "    print('self-dedup', dedup_num_rcds_old, dedup_num_rcds_google)\n",
    "    # merge two data\n",
    "    dict_google = merge_dict(dict_old, dict_google)\n",
    "    return dict_google\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total media size:\t 339501557\n",
      "total file size:\t 339501557\n",
      "media-size / file-size:\t 1.0\n",
      "total media size:\t 2910934\n",
      "total file size:\t 2910934\n",
      "media-size / file-size:\t 1.0\n",
      "old #fname 8\n",
      "old #rcrds 8\n",
      "google #fname 3\n",
      "google #rcrds 3\n",
      "total #rcrd 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/p4g508r161q9jq5g8xlqnhs00000gn/T/ipykernel_38319/703979076.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta_files['ext_lower'] = df_meta_files['ext'].str.lower()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2e4dededc644319d9729212f514ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03f4492d37e437693c8d04831c4b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self-dedup 0 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4edd958e30494d8968b32aaab6e912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#google key: old:3, new: 11, added: 8\n",
      "len_not_same_name 8\n",
      "len_same_name 0\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_meta_old = pd.read_json(\"/Volumes/ssd-0/old-photo-organize/MyPhotoVideoAudio-meta.json\")\n",
    "rcrd_meta_old_media = get_media_file_meta_df(df_meta_old)\n",
    "\n",
    "df_meta_google = pd.read_json(\"/Volumes/ssd-0/old-photo-organize/google-photo-dump-20230220-meta.json\")\n",
    "rcrd_meta_google_media = get_media_file_meta_df(df_meta_google)\n",
    "\n",
    "dict_all = merge_files(rcrd_meta_old_media, rcrd_meta_google_media)\n",
    "\n",
    "total_records = sum([len(dict_all[key]) for key in dict_all.keys()])\n",
    "print(total_records)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proba duplicate imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Wallace_at_University_of_Alabama_edit2.jpg', 1), ('DSC07994.JPG', 1), ('IMG_0867.MOV', 1), ('P5201477.AVI', 1), ('PB030168.JPG', 1), ('P5201458.AVI', 1), ('P1250290.AVI', 1), ('PA280004.JPG', 1), ('P1250286.AVI', 1), ('PA290079.JPG', 1), ('PA290077.JPG', 1)]\n"
     ]
    }
   ],
   "source": [
    "key_records = [(key, len(dict_all[key])) for key in dict_all.keys()]\n",
    "key_records = sorted(key_records, key=lambda x:x[1], reverse=True)\n",
    "print(key_records[:100])\n",
    "#71380\n",
    "#68984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'IMG_0430.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tt/p4g508r161q9jq5g8xlqnhs00000gn/T/ipykernel_38319/3606774991.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IMG_0430.JPG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'IMG_0430.JPG'"
     ]
    }
   ],
   "source": [
    "dict_all['IMG_0430.JPG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db332fe5bc5ebb4742063ca4b87cff3f471d2a090567720d31fef0635138e626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
